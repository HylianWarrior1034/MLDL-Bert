{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0LoYpdMinoD",
        "outputId": "63360c41-56d8-464c-d374-73ca56dd9025"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.0/7.0 MB 53.9 MB/s eta 0:00:00\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 224.5/224.5 kB 28.5 MB/s eta 0:00:00\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 105.2 MB/s eta 0:00:00\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 519.2/519.2 kB 10.9 MB/s eta 0:00:00\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.1)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.0.0+cu118)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (4.5.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.11.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (16.0.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.25.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.11.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.28.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2022.10.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.14.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (23.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (4.65.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.13.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (6.0)\n",
            "Collecting protobuf<=3.20.2\n",
            "  Downloading protobuf-3.20.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 17.8 MB/s eta 0:00:00\n",
            "Collecting sentencepiece!=0.1.92,>=0.1.91\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 62.3 MB/s eta 0:00:00\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers[sentencepiece]) (4.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers[sentencepiece]) (2023.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2022.12.7)\n",
            "Installing collected packages: sentencepiece, protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "Successfully installed protobuf-3.20.2 sentencepiece-0.1.99\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 880.6/880.6 kB 14.8 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2022.10.31)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.65.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py): started\n",
            "  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895259 sha256=3761696e756e2e69c4d4dbcf65d56564fcc911975389490fd6d05b53475f18db\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.0.53\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting deep-translator\n",
            "  Downloading deep_translator-1.10.1-py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (2.27.1)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.0.12)\n",
            "Installing collected packages: deep-translator\n",
            "Successfully installed deep-translator-1.10.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.12.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install transformers\n",
        "pip install torchmetrics\n",
        "pip install transformers[sentencepiece]\n",
        "pip install sacremoses\n",
        "pip install -U deep-translator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer\n",
        "from transformers import pipeline, Pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "from deep_translator import GoogleTranslator\n"
      ],
      "metadata": {
        "id": "mV87LvPylWJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing all datasets!\n",
        "\n"
      ],
      "metadata": {
        "id": "9tzGSUNYmd7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ### just a one time thing, cleaning the datasets up: \n",
        "\n",
        "# kor_dataset_train = pd.read_csv(\"/content/MLDL_korean_dataset_train.csv\").drop(\"Unnamed: 0\", axis = 1)[[\"comments\", \"contain_gender_bias\"]]\n",
        "# kor_dataset_test = pd.read_csv(\"/content/MLDL_korean_dataset_test.csv\").drop(\"Unnamed: 0\", axis = 1)[[\"comments\", \"contain_gender_bias\"]]\n",
        "\n",
        "# kor_dataset_train = kor_dataset_train.rename(columns = {\"comments\": \"text\", \"contain_gender_bias\": \"misogynous\"})\n",
        "# kor_dataset_test = kor_dataset_test.rename(columns = {\"comments\": \"text\", \"contain_gender_bias\": \"misogynous\"})\n",
        "\n",
        "# kor_dataset_train['misogynous'] = (kor_dataset_train['misogynous']).astype(int)\n",
        "# kor_dataset_test['misogynous'] = (kor_dataset_test['misogynous']).astype(int)\n",
        "\n",
        "# kor_dataset_train.to_csv(\"kor_dataset_train_clean.csv\")\n",
        "# kor_dataset_test.to_csv(\"kor_dataset_test_clean.csv\")\n",
        "\n",
        "# ####################################################################################################################\n",
        "\n",
        "# it_dataset_train = pd.read_csv(\"/content/it_training_anon.tsv\", sep = '\\t')[['text', 'misogynous']]\n",
        "# it_dataset_test = pd.read_csv(\"/content/it_testing_labeled_anon.tsv\", sep = '\\t')[['text', 'misogynous']]\n",
        "\n",
        "# words_remove = [\"<MENTION_1>\", \"<MENTION_2>\", \"<MENTION_3>\"]\n",
        "\n",
        "# for i in range(len(it_dataset_train['text'])):\n",
        "#     text = it_dataset_train['text'][i]\n",
        "#     it_dataset_train['text'][i] = \" \".join([word for word in text.split() if word not in words_remove])\n",
        "\n",
        "# for i in range(len(it_dataset_test['text'])):\n",
        "#     it_dataset_test['text'][i] = \" \".join([word for word in it_dataset_test['text'][i].split() if word not in words_remove])\n",
        "\n",
        "# it_dataset_train.to_csv(\"it_dataset_train_clean.csv\")\n",
        "# it_dataset_test.to_csv(\"it_dataset_test_clean.csv\")\n",
        "\n",
        "# ####################################################################################################################\n",
        "\n",
        "# en_dataset_train = pd.read_csv(\"/content/en_training_anon.tsv\", sep = '\\t')[['text', 'misogynous']]\n",
        "# en_dataset_test = pd.read_csv(\"/content/en_testing_labeled_anon.tsv\", sep = '\\t')[['text', 'misogynous']]\n",
        "\n",
        "# words_remove = [\"<MENTION_1>\", \"<MENTION_2>\", \"<MENTION_3>\"]\n",
        "\n",
        "# for i in range(len(en_dataset_train['text'])):\n",
        "#     en_dataset_train['text'][i] = \" \".join([word for word in en_dataset_train['text'][i].split() if word not in words_remove])\n",
        "\n",
        "# for i in range(len(en_dataset_test['text'])):\n",
        "#     en_dataset_test['text'][i] = \" \".join([word for word in en_dataset_test['text'][i].split() if word not in words_remove])\n",
        "\n",
        "# en_dataset_train.to_csv(\"en_dataset_train_clean.csv\")\n",
        "# en_dataset_test.to_csv(\"en_dataset_test_clean.csv\")\n",
        "\n",
        "# ####################################################################################################################\n",
        "\n",
        "# en_dataset_alt = pd.read_csv(\"/content/final_labels.csv\")[['body', 'level_1']]\n",
        "# en_dataset_alt = en_dataset_alt.rename(columns={\"level_1\": \"misogynous\"})\n",
        "# en_dataset_alt[\"misogynous\"] = (en_dataset_alt[\"misogynous\"] == \"Misogynistic\").astype(int)\n",
        "# en_dataset_alt.to_csv(\"en_dataset_alt.csv\")"
      ],
      "metadata": {
        "id": "oMx3dk7tmalL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now augment the dataset. We use random insertion and back translation. We avoid using random replacement as augmentation since the word that is replaced may cause the entire meaning of the sentence to change. We use huggingface pipelines for these augmentations. \n",
        "\n",
        "Most importantly, we don't augment in the Dataset class since the pipeline needs to know the source language beforehand for the translation model. \n"
      ],
      "metadata": {
        "id": "ovCoL3UzasgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(df, source_language): \n",
        "    texts = list(df[\"text\"])\n",
        "    batch_size = 8\n",
        "\n",
        "    df_texts = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        print(i)\n",
        "        if np.random.random() > 0.7:\n",
        "            t_text = GoogleTranslator(source=source_language, target='de').translate_batch(texts[i:i+batch_size]) \n",
        "            t_text2 = GoogleTranslator(source='de', target=source_language).translate_batch(t_text) \n",
        "            df_texts += t_text2\n",
        "        else:\n",
        "            df_texts += (texts[i:i+batch_size])\n",
        "  \n",
        "    df[\"text\"] = df_texts\n",
        "    return df\n",
        "\n",
        "def process_text(text):\n",
        "    orig_text_list = text.split()\n",
        "    len_input = len(orig_text_list)\n",
        "    #Random index where we want to insert the word except at the start or end\n",
        "    try:\n",
        "        rand_idx = np.random.randint(1,len_input-2)\n",
        "      \n",
        "    except: \n",
        "        rand_idx = 1\n",
        "\n",
        "    new_text_list = orig_text_list[:rand_idx] + ['[MASK]'] + orig_text_list[rand_idx:]\n",
        "    new_mask_sent = ' '.join(new_text_list)\n",
        "    return new_mask_sent\n",
        "     \n",
        "def insert(df): \n",
        "    texts = df[\"text\"]\n",
        "    unmasker = pipeline('fill-mask', model='bert-base-multilingual-cased')\n",
        "    batch_size = 8\n",
        "\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        if np.random.random() > 0.5:\n",
        "            temp_texts = list(map(process_text, texts[i:i+batch_size]))\n",
        "\n",
        "            augmented_text_list = unmasker(temp_texts)\n",
        "            augmented_text = [text[0]['sequence'] for text in augmented_text_list]\n",
        "            texts[i:i+batch_size] = augmented_text\n",
        "\n",
        "    df[\"text\"] = texts\n",
        "    return df"
      ],
      "metadata": {
        "id": "IuUllg-7bNaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyPipeline(Pipeline):\n",
        "    def _sanitize_parameters(self, **kwargs):\n",
        "        preprocess_kwargs = {}\n",
        "        if \"maybe_arg\" in kwargs:\n",
        "            preprocess_kwargs[\"maybe_arg\"] = kwargs[\"maybe_arg\"]\n",
        "        return preprocess_kwargs, {}, {}\n",
        "\n",
        "    def preprocess(self, df, maybe_arg=2):\n",
        "        df[\"text\"] = df[\"text\"].apply(lambda x: process_text(x))\n",
        "        model_input = torch.Tensor(df[\"text\"])\n",
        "        return {\"model_input\": model_input}\n",
        "\n",
        "    def _forward(self, model_inputs):\n",
        "        outputs = self.model(**model_inputs)\n",
        "        # Maybe {\"logits\": Tensor(...)}\n",
        "        return outputs\n",
        "\n",
        "    def postprocess(self, model_outputs):\n",
        "        return model_outputs[\"model_input\"]"
      ],
      "metadata": {
        "id": "_ZO7V8_YmHR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset): \n",
        "  def __init__(self, df):\n",
        "    self.labels = list(df['misogynous'])\n",
        "    self.texts = [tokenizer(text, padding='max_length', \n",
        "                            max_length = 512, truncation=True,\n",
        "                            return_tensors=\"pt\") for text in df['text']]\n",
        "\n",
        "  def classes(self):\n",
        "      return self.labels\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.labels)\n",
        "\n",
        "  def get_batch_labels(self, idx):\n",
        "      # Fetch a batch of labels\n",
        "      return np.array(self.labels[idx])\n",
        "\n",
        "  def get_batch_texts(self, idx):\n",
        "      # Fetch a batch of inputs\n",
        "      return self.texts[idx]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      batch_texts = self.get_batch_texts(idx)\n",
        "      batch_y = self.get_batch_labels(idx)\n",
        "\n",
        "      return batch_texts, batch_y"
      ],
      "metadata": {
        "id": "-yvHAUq4uaiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unaugmented dataset\n",
        "\n",
        "kor_dataset_train = pd.read_csv(\"/content/kor_dataset_train_clean.csv\").drop(\"Unnamed: 0\", axis = 1)\n",
        "kor_dataset_test = pd.read_csv(\"/content/kor_dataset_test_clean.csv\").drop(\"Unnamed: 0\", axis = 1)\n",
        "it_dataset_train = pd.read_csv(\"/content/it_dataset_train_clean.csv\")\n",
        "it_dataset_test = pd.read_csv(\"/content/it_dataset_test_clean.csv\")\n",
        "en_dataset_train = pd.read_csv(\"/content/en_dataset_train_clean.csv\")\n",
        "en_dataset_test = pd.read_csv(\"/content/en_dataset_test_clean.csv\")\n",
        "\n",
        "df2 = pd.concat([kor_dataset_train, it_dataset_train, en_dataset_train, kor_dataset_test, it_dataset_test, en_dataset_test]).drop(\"Unnamed: 0\", axis = 1)\n",
        "print(len(df2))\n",
        "# df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42), [int(.8*len(df)), int(.9*len(df))])\n",
        "# df_train = df_train.reset_index(drop = True)\n",
        "# df_val = df_val.reset_index(drop = True)\n",
        "# df_test = df_test.reset_index(drop = True)"
      ],
      "metadata": {
        "id": "kwDK7IrQwAgN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a9d6fa5-48c0-4dc8-90b2-a188160b38f8"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18367\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Augmented dataset \n",
        "\n",
        "df = pd.read_csv(\"/content/masterdf.csv\").drop(\"Unnamed: 0\", axis=1)\n",
        "\n",
        "\n",
        "df = df.loc[(df['text'].notna())]\n",
        "df = pd.concat([df, df2]).drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42), [int(.8*len(df)), int(.9*len(df))])\n",
        "df_train = df_train.reset_index(drop = True)\n",
        "df_val = df_val.reset_index(drop = True)\n",
        "df_test = df_test.reset_index(drop = True)"
      ],
      "metadata": {
        "id": "8N6tXsV2-J_1"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel\n",
        "\n",
        "class BertClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, dropout=0.5):\n",
        "\n",
        "        super(BertClassifier, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear = nn.Linear(768, 1)\n",
        "        self.ff1 = nn.Linear(768, 360)\n",
        "        self.ff2 = nn.Linear(360, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_id, mask):\n",
        "\n",
        "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "        linear_output = self.linear(dropout_output)\n",
        "        final_layer = self.sigmoid(linear_output)\n",
        "\n",
        "        return final_layer"
      ],
      "metadata": {
        "id": "Nbml532qBUPV"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "\n",
        "epochs = 2\n",
        "model = BertClassifier()\n",
        "learning_rate = 1e-5\n",
        "\n",
        "train, val = Dataset(df_train), Dataset(df_val)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train, batch_size=24, shuffle=True)\n",
        "val_dataloader = torch.utils.data.DataLoader(val, batch_size=8)\n",
        "\n",
        "gpu_boole = torch.cuda.is_available()\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = Adam(model.parameters(), lr= learning_rate)\n",
        "\n",
        "if gpu_boole:\n",
        "    model = model.cuda()\n",
        "\n",
        "for epoch_num in range(epochs):\n",
        "\n",
        "    total_acc_train = 0\n",
        "    total_loss_train = 0\n",
        "\n",
        "    for train_input, train_label in tqdm(train_dataloader):\n",
        "        mask = train_input['attention_mask']\n",
        "        input_id = train_input['input_ids'].squeeze(1)\n",
        "\n",
        "        if gpu_boole:\n",
        "          train_label = train_label.cuda()\n",
        "          mask, input_id = mask.cuda(), input_id.cuda()\n",
        "\n",
        "        output = model(input_id, mask)\n",
        "        \n",
        "        batch_loss = criterion(output.squeeze(1), train_label.float())\n",
        "        total_loss_train += batch_loss.item()\n",
        "        \n",
        "        acc = (output.argmax(dim=1) == train_label).sum().item()\n",
        "        total_acc_train += acc\n",
        "\n",
        "        model.zero_grad()\n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    total_acc_val = 0\n",
        "    total_loss_val = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for val_input, val_label in val_dataloader:\n",
        "            mask = val_input['attention_mask']\n",
        "            input_id = val_input['input_ids'].squeeze(1)\n",
        "\n",
        "            if gpu_boole:\n",
        "                val_label = val_label.cuda()\n",
        "                mask, input_id = mask.cuda(), input_id.cuda()\n",
        "\n",
        "            output = model(input_id, mask)\n",
        "\n",
        "            batch_loss = criterion(output.squeeze(1), val_label.float())\n",
        "            total_loss_val += batch_loss.item()\n",
        "            \n",
        "            acc = (output.argmax(dim=1) == val_label).sum().item()\n",
        "            total_acc_val += acc\n",
        "    \n",
        "    print(\n",
        "        f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(df_train): .3f} \\\n",
        "        | Train Accuracy: {total_acc_train / len(df_train): .3f} \\\n",
        "        | Val Loss: {total_loss_val / len(df_val): .3f} \\\n",
        "        | Val Accuracy: {total_acc_val / len(df_val): .3f}')\n",
        "    \n",
        "    torch.cuda.empty_cache()\n",
        "          "
      ],
      "metadata": {
        "id": "TbhVRm_tDHeh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cea47bbf-ec3b-4812-931a-fe443523a10b"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 1008/1008 [34:03<00:00,  2.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 1 | Train Loss:  0.015         | Train Accuracy:  0.683         | Val Loss:  0.034         | Val Accuracy:  0.692\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1008/1008 [34:07<00:00,  2.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 2 | Train Loss:  0.008         | Train Accuracy:  0.683         | Val Loss:  0.027         | Val Accuracy:  0.692\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1008/1008 [34:08<00:00,  2.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 3 | Train Loss:  0.003         | Train Accuracy:  0.683         | Val Loss:  0.032         | Val Accuracy:  0.692\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics.classification import BinaryF1Score\n",
        "\n",
        "test = Dataset(df_test)\n",
        "\n",
        "test_dataloader = torch.utils.data.DataLoader(test, batch_size=8)\n",
        "\n",
        "gpu_boole = torch.cuda.is_available()\n",
        "\n",
        "metric = BinaryF1Score()\n",
        "\n",
        "if gpu_boole:\n",
        "    metric.cuda()\n",
        "    model.cuda()\n",
        "\n",
        "total_acc_test = 0\n",
        "f1_score = 0\n",
        "count = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "  for test_input, test_label in test_dataloader:\n",
        "      mask = test_input['attention_mask']\n",
        "      input_id = test_input['input_ids'].squeeze(1)\n",
        "\n",
        "      if gpu_boole:\n",
        "        test_label = test_label.cuda()\n",
        "        mask, input_id = mask.cuda(), input_id.cuda()\n",
        "\n",
        "      output = model(input_id, mask)\n",
        "\n",
        "      f1_score += metric(output.squeeze(1), test_label).sum().item()\n",
        "      count += 1\n",
        "\n",
        "      acc = (output.argmax(dim=1) == test_label).sum().item()\n",
        "      total_acc_test += acc\n",
        "\n",
        "print(f'Test Accuracy: {total_acc_test / len(df_test): .3f}')\n",
        "print(f'F1 Score: {f1_score / count: .3f}')"
      ],
      "metadata": {
        "id": "gpdW6DIyLIsU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6d22e76-e391-460e-a2ea-df344b7a6a4d"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy:  0.674\n",
            "F1 Score:  0.817\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "zVXxMmIv1Nm5"
      },
      "execution_count": 66,
      "outputs": []
    }
  ]
}